# AndrewNG-Coursera
<h1>Andrew NG Machine Learning - Coursera Course</h1>
<h3>Syllabus</h3> 
1.<b> Introduction</b><br>
Welcome to Machine Learning!<br>
Welcome<br>
What is Machine Learning?<br>
Supervised Learning<br>
Unsupervised Learning<br>

2.<b> Linear Regression with One Variable</b><br>
Model Representation<br>
Cost Function<br>
Cost Function - Intuition I<br>
Cost Function - Intuition II<br>
Gradient Descent<br>
Gradient Descent Intuition<br>
Gradient Descent For Linear Regression<br>

3.<b> Linear Algebra Review</b><br>
Matrices and Vectors<br>
Addition and Scalar Multiplication<br>
Matrix Vector Multiplication<br>
Matrix Matrix Multiplication<br>
Matrix Multiplication Properties<br>
Inverse and Transpose<br>

4.<b> Linear Regression with Multiple Variables</b><br>
Multiple Features<br>
Gradient Descent for Multiple Variables<br>
Gradient Descent in Practice I - Feature Scaling<br>
Gradient Descent in Practice II - Learning Rate<br>
Features and Polynomial Regression<br>
Normal Equation<br>
Normal Equation Noninvertibility<br>
Working on and Submitting Programming Assignments<br>

5.<b> Octave/Matlab Tutorial</b><br>
Basic Operations<br>
Moving Data Around<br>
Computing on Data<br>
Plotting Data<br>
Control Statements: for, while, if statement<br>
Vectorization<br>

6.<b> Logistic Regression</b><br>
Classification<br>
Hypothesis Representation<br>
Decision Boundary<br>
Cost Function<br>
Simplified Cost Function and Gradient Descent<br>
Advanced Optimization<br>
Multiclass Classification: One-vs-all<br>

7.<b> Regularization</b><br>
The Problem of Overfitting<br>
Cost Function<br>
Regularized Linear Regression<br>
Regularized Logistic Regression<br>

8.<b> Neural Networks: Representation</b><br>
Non-linear Hypotheses<br>
Neurons and the Brain<br>
Model Representation I<br>
Model Representation II<br>
Examples and Intuitions I<br>
Examples and Intuitions II<br>
Multiclass Classification<br>

9.<b> Neural Networks: Learning</b><br>
Cost Function<br>
Backpropagation Algorithm<br>
Backpropagation Intuition<br>
Implementation Note: Unrolling Parameters<br>
Gradient Checking<br>
Random Initialization<br>
Putting It Together<br>
Autonomous Driving<br>

10.<b> Advice for Applying Machine Learning</b><br>
Deciding What to Try Next<br>
Evaluating a Hypothesis<br>
Model Selection and Train/Validation/Test Sets<br>
Diagnosing Bias vs.<b> Variance<br></b><br>
Regularization and Bias/Variance<br>
Learning Curves<br>
Deciding What to Do Next Revisited<br>

11.<b> Machine Learning System Design</b><br>
Prioritizing What to Work On<br>
Error Analysis<br>
Error Metrics for Skewed Classes<br>
Trading Off Precision and Recall<br>
Data For Machine Learning<br>

12.<b> Support Vector Machines</b><br>
Optimization Objective<br>
Large Margin Intuition<br>
Mathematics Behind Large Margin Classification<br>
Kernels I<br>
Kernels II<br>
Using An SVM<br>

13.<b> Unsupervised Learning</b><br>
Unsupervised Learning: Introduction<br>
K-Means Algorithm<br>
Optimization Objective<br>
Random Initialization<br>
Choosing the Number of Clusters<br>

14.<b> Dimensionality Reduction</b><br>
Motivation I: Data Compression<br>
Motivation II: Visualization<br>
Principal Component Analysis Problem Formulation<br>
Principal Component Analysis Algorithm<br>
Reconstruction from Compressed Representation<br>
Choosing the Number of Principal Components<br>
Advice for Applying PCA<br>

15.<b> Anomaly Detection</b><br>
Problem Motivation<br>
Gaussian Distribution<br>
Algorithm<br>
Developing and Evaluating an Anomaly Detection System<br>
Anomaly Detection vs.<b> Supervised Learning<br></b><br>
Choosing What Features to Use<br>
Multivariate Gaussian Distribution<br>
Anomaly Detection using the Multivariate Gaussian Distribution<br>

16.<b> Recommender Systems</b><br>
Problem Formulation<br>
Content Based Recommendations<br>
Collaborative Filtering<br>
Collaborative Filtering Algorithm<br>
Vectorization: Low Rank Matrix Factorization<br>
Implementational Detail: Mean Normalization<br>

17.<b> Large Scale Machine Learning</b><br>
Learning With Large Datasets<br>
Stochastic Gradient Descent<br>
Mini-Batch Gradient Descent<br>
Stochastic Gradient Descent Convergence<br>
Online Learning<br>
Map Reduce and Data Parallelism<br>

18.<b> Application Example: Photo OCR</b><br>
Problem Description and Pipeline<br>
Sliding Windows<br>
Getting Lots of Data and Artificial Data<br>
Ceiling Analysis: What Part of the Pipeline to Work on Next<br>
Summary and Thank You<br>
